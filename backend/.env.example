# ============================================
# LiteLLM Configuration
# ============================================
# Your LiteLLM server URL (e.g., OpenRouter, self-hosted LiteLLM, or proxy)
# Example: https://api.openrouter.ai or https://proxyllm.ximplify.id
LITELLM_API_URL=https://api.openrouter.ai

# Your LiteLLM/OpenRouter API key
# Example: sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
LITELLM_API_KEY=your_litellm_api_key_here

# Authorization header name (usually "Authorization")
# Default: "Authorization"
LITELLM_HEADER_NAME=Authorization

# Authorization scheme (usually "Bearer")
# Default: "Bearer"
LITELLM_AUTH_SCHEME=Bearer

# ============================================
# Model Configuration
# ============================================
# Model to use for extraction (works with any model supported by LiteLLM)
# Examples:
#   - google_ai/gemini/gemini-2.5-flash
#   - openrouter/google/gemini-2.5-flash
#   - openrouter/google/gemini-2.5-pro
#   - openrouter/anthropic/claude-3.5-sonnet
# Default: openrouter/google/gemini-2.5-flash
EXTRACTION_MODEL=openrouter/google/gemini-2.5-flash

# ============================================
# Supabase Configuration
# ============================================
# Your Supabase project URL
# Example: https://xxxxxxxxxxxxx.supabase.co
SUPABASE_URL=https://your-project.supabase.co

# Your Supabase service role key (keep this secret!)
# Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_here

# ============================================
# Server Configuration
# ============================================
# Server host (0.0.0.0 allows external connections)
HOST=0.0.0.0

# Server port
PORT=8000

# Debug mode (True for development, False for production)
DEBUG=True

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================
# Document Analysis Configuration
# ============================================
# Minimum confidence threshold for field extraction (0.0 to 1.0)
# Default: 0.6 (60%)
MIN_CONFIDENCE_THRESHOLD=0.6

# ============================================
# Parallel Processing Configuration
# ============================================
# Maximum number of pages to process concurrently
# Default: 10
PDF_PROCESSING_MAX_WORKERS=10

# ============================================
# Optional: LangSmith Tracing
# ============================================
# Enable LangSmith tracing for LLM calls
# Set to "true" to enable, "false" to disable
# Default: false
LANGSMITH_TRACING=false

# ============================================
# YOLO Signature Detection Configuration
# ============================================
# Enable YOLO-based signature detection
# Set to "true" to enable, "false" to disable
# Default: true
YOLO_SIGNATURE_ENABLED=true

# Path to YOLO model file (relative to backend directory)
YOLO_MODEL_PATH=models/signature_detector.pt

# Confidence threshold for signature detection (0.0 to 1.0)
# Default: 0.5
YOLO_CONFIDENCE_THRESHOLD=0.5

# IoU threshold for non-maximum suppression (0.0 to 1.0)
# Default: 0.45
YOLO_IOU_THRESHOLD=0.45

# Use GPU for YOLO inference (requires CUDA-enabled GPU)
# Set to "true" if GPU available, "false" for CPU
# Default: false
YOLO_USE_GPU=false

# ============================================
# Instructions
# ============================================
# 1. Copy this file to .env: cp .env.example .env
# 2. Fill in your actual API keys and configuration values
# 3. Never commit .env to version control (it's in .gitignore)
# 4. Configure LITELLM_API_URL and LITELLM_API_KEY for your LiteLLM provider
# 5. Update EXTRACTION_MODEL to use your preferred model
